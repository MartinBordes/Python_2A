{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svbaEInHvuk7"
   },
   "source": [
    "# 1 - Préparation du scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnRnoYwgx9-Q"
   },
   "source": [
    "## 1.1 - Importation des modules pertinents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "3m8ouQ2vvtzX",
    "outputId": "8b8f6f02-e0da-464b-ad94-139490971f10"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgjkxilGyBsV"
   },
   "source": [
    "## 1.2 - Création de la liste qui contiendra les donnés scrapées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LQQ-e2y5vtzY",
    "outputId": "2395ee12-764d-47cf-c4f8-de2af93d1f78"
   },
   "outputs": [],
   "source": [
    "tweets_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcw3qBYsyI6x"
   },
   "source": [
    "## 1.3 - Choix des périodes pertinentes pour le scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUZbjHTWyNig"
   },
   "source": [
    "Avant de commencer à scraper, il faut déterminer les dates pertinentes pour chaque film. Pour pouvoir répondre à notre problématique, il convient de comparer un échantillon de tweets écrits avant la sortie du film et un échantillon de tweets écrits après. Pour nous assurer que les tweets scrapés portent bien sur le contenu du film, nous avons décidé de retenir les périodes suivant la publication des différents trailers des films.\\\n",
    "\\\n",
    "Dans le cas de Dune, le premier trailer du film est publié sur Youtube le 09 septembre 2020 et le second le 22 juillet 2021. Nous avons donc également scrapé sur des périodes de deux semaines suivant la sortie de chaque trailer et sur une période de cinq semaines suivant la sortie du film en salles le 08 octobre 2021 (donc jusqu’au 13 novembre 2021).\n",
    "\\\n",
    "\\\n",
    "Dans le cas de Space Jam 2, le premier trailer du film est publié sur Youtube le 03 avril 2021 et le second le 09 juin de la même année. Nous avons donc scrapé sur des périodes de deux semaines suivant la sortie de chaque trailer et sur une période de cinq semaines suivant la sortie du film en salles le 03 juillet 2021 (donc jusqu’au 08 août 2021).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LWR0INyjv-dQ"
   },
   "outputs": [],
   "source": [
    "periods_dune = [pd.date_range(start='2020-09-09', end='2020-09-23').strftime(\"%Y-%m-%d\").tolist(),\n",
    "                pd.date_range(start='2021-07-22', end='2021-08-05').strftime(\"%Y-%m-%d\").tolist(),\n",
    "                pd.date_range(start='2021-10-08', end='2021-11-13').strftime(\"%Y-%m-%d\").tolist()]\n",
    "\n",
    "periods_space= [pd.date_range(start='2021-04-03', end='2021-04-17').strftime(\"%Y-%m-%d\").tolist(),\n",
    "                pd.date_range(start='2021-06-09', end='2021-06-23').strftime(\"%Y-%m-%d\").tolist(),\n",
    "                pd.date_range(start='2021-07-03', end='2021-08-08').strftime(\"%Y-%m-%d\").tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JdZYVnwwF6t"
   },
   "source": [
    "# 2 - Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIEoZTNYxt9j"
   },
   "source": [
    "## 2.1 - Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK1rTI74wNtg"
   },
   "source": [
    "Pour scraper, nous n'avons pas recours à une API mais à un module appelé *TwitterSearchScraper* dont les fonctionnalités sont suffisantes pour nos besoins. On utilise donc TwitterSearchScraper pour obtenir tous les tweets contenant un string choisi durant une période choisie.\\\n",
    "\\\n",
    "Pour des raisons tenant aux limitations quantitativs imposées par Twitter au scraping ne faisant pas appel à une API, nous ne scrapons qu'au plus 500 tweets par jour.\n",
    "\\\n",
    "\\\n",
    "Parmi toutes les données que le module permet de récolter, nous décidons de ne relever que le film concerné, la période durant laquelle le tweet a été écrit, son contenu, sa date, son numéro (pour bonne mesure) et la langue (ici l'Anglais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LKQMLRGEv-RG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/search?f=live&lang=en&q=dune+movie+since%3A2020-09-09+until%3A2020-09-10+lang%3Aen&src=spelling_expansion_revert_click: unable to find guest token\n",
      "4 requests to https://twitter.com/search?f=live&lang=en&q=dune+movie+since%3A2020-09-09+until%3A2020-09-10+lang%3Aen&src=spelling_expansion_revert_click failed, giving up.\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/search?f=live&lang=en&q=dune+movie+since%3A2020-09-09+until%3A2020-09-10+lang%3Aen&src=spelling_expansion_revert_click failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c86e619cf051>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtime_range\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mperiods\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msntwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTwitterSearchScraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{film} movie since:{time_range[k]} until:{time_range[k+1]} lang:en\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# On scrape 500 tweets par jour\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\snscrape\\modules\\twitter.py\u001b[0m in \u001b[0;36mget_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    588\u001b[0m                         \u001b[1;32mdel\u001b[0m \u001b[0mpaginationParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_search_mode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_api_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://api.twitter.com/2/search/adaptive.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaginationParams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_instructions_to_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\snscrape\\modules\\twitter.py\u001b[0m in \u001b[0;36m_iter_api_data\u001b[1;34m(self, endpoint, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Retrieving scroll page {cursor}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m                         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_api_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreqParams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\snscrape\\modules\\twitter.py\u001b[0m in \u001b[0;36m_get_api_data\u001b[1;34m(self, endpoint, params)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_api_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_guest_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apiHeaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponseOkCallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_api_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\snscrape\\modules\\twitter.py\u001b[0m in \u001b[0;36m_ensure_guest_token\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_guestTokenManager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Retrieving guest token'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_baseUrl\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'User-Agent'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_userAgent\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponseOkCallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_guest_token_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m \u001b[1;33m:=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'document\\.cookie = decodeURIComponent\\(\"gt=(\\d+); Max-Age=10800; Domain=\\.twitter\\.com; Path=/; Secure\"\\);'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found guest token in HTML'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\snscrape\\base.py\u001b[0m in \u001b[0;36m_get\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_post\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\snscrape\\base.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects)\u001b[0m\n\u001b[0;32m    198\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{self._retries + 1} requests to {req.url} failed, giving up.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfatal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mScraperException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reached unreachable code'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mScraperException\u001b[0m: 4 requests to https://twitter.com/search?f=live&lang=en&q=dune+movie+since%3A2020-09-09+until%3A2020-09-10+lang%3Aen&src=spelling_expansion_revert_click failed, giving up."
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "for film , periods in [('dune',periods_dune),('space jam',periods_space)]:\n",
    "    for time_range in periods :\n",
    "        for k in range(len(time_range)-1):\n",
    "            for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f\"{film} movie since:{time_range[k]} until:{time_range[k+1]} lang:en\").get_items()):\n",
    "                if i>=500: # On scrape 500 tweets par jour\n",
    "                    break\n",
    "                tweets_list.append([film, time_range, tweet.content, tweet.date, tweet.id, tweet.lang])\n",
    "print(f\"Temps d'exécution : {time.strftime('%H:%M:%S', time.gmtime(time.time()-t))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBxTB8Vjxw4Z"
   },
   "source": [
    "# 2.2 - Mise en forme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXZ_iQlexlv_"
   },
   "source": [
    "On crée un dataframe à partir de ce qu'on a scrapé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZFnUxRtv-EX"
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(tweets_list, columns=['Film', 'Time Range', 'Text', 'Datetime', 'Tweet Id', 'Language'])\n",
    "end=time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xETcnlSmvtza"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUaFGzSJvtza",
    "outputId": "4910c95c-926b-4426-bc94-5a01dd24fffe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Film</th>\n",
       "      <th>Time Range</th>\n",
       "      <th>Text</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dune</td>\n",
       "      <td>[2020-09-09, 2020-09-10, 2020-09-11, 2020-09-12, 2020-09-13, 2020-09-14, 2020-09-15, 2020-09-16, 2020-09-17, 2020-09-18, 2020-09-19, 2020-09-20, 2020-09-21, 2020-09-22, 2020-09-23]</td>\n",
       "      <td>I wanna see Dune but the lack of Muslim and MENA actors when the novel supposedly references the middle-east and Islam is.... disappointing. Would have been a real good bit of representation for once, especially in a big Hollywood movie</td>\n",
       "      <td>2020-09-09 23:59:53+00:00</td>\n",
       "      <td>1303845614120304641</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dune</td>\n",
       "      <td>[2020-09-09, 2020-09-10, 2020-09-11, 2020-09-12, 2020-09-13, 2020-09-14, 2020-09-15, 2020-09-16, 2020-09-17, 2020-09-18, 2020-09-19, 2020-09-20, 2020-09-21, 2020-09-22, 2020-09-23]</td>\n",
       "      <td>I might be the only person that knows fuckall about Dune but damn that trailer looks great and I love Denis Villenueve. That dude hasn't made a bad movie that I've seen and I've seen almost all of them.</td>\n",
       "      <td>2020-09-09 23:59:48+00:00</td>\n",
       "      <td>1303845592548941824</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dune</td>\n",
       "      <td>[2020-09-09, 2020-09-10, 2020-09-11, 2020-09-12, 2020-09-13, 2020-09-14, 2020-09-15, 2020-09-16, 2020-09-17, 2020-09-18, 2020-09-19, 2020-09-20, 2020-09-21, 2020-09-22, 2020-09-23]</td>\n",
       "      <td>When people try to claim Dune is some higher level, intelligent adult sci-fi and I wonder if I watched the same movie? #Dune https://t.co/rHiAbRLQL0</td>\n",
       "      <td>2020-09-09 23:59:43+00:00</td>\n",
       "      <td>1303845572814790656</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dune</td>\n",
       "      <td>[2020-09-09, 2020-09-10, 2020-09-11, 2020-09-12, 2020-09-13, 2020-09-14, 2020-09-15, 2020-09-16, 2020-09-17, 2020-09-18, 2020-09-19, 2020-09-20, 2020-09-21, 2020-09-22, 2020-09-23]</td>\n",
       "      <td>@nosleeptilltaco I never read Dune but Im v glad yall that did are getting a good movie</td>\n",
       "      <td>2020-09-09 23:59:40+00:00</td>\n",
       "      <td>1303845558373756929</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dune</td>\n",
       "      <td>[2020-09-09, 2020-09-10, 2020-09-11, 2020-09-12, 2020-09-13, 2020-09-14, 2020-09-15, 2020-09-16, 2020-09-17, 2020-09-18, 2020-09-19, 2020-09-20, 2020-09-21, 2020-09-22, 2020-09-23]</td>\n",
       "      <td>@dunemovie The mix of styles is really strange , the music is  not matching the clip or the ambiance , almost disgusting me , also the casting isn't that great imo for a movie with such ambition .. I have that feeling telling me it's gonna be an okay movie and not more</td>\n",
       "      <td>2020-09-09 23:59:39+00:00</td>\n",
       "      <td>1303845555169374214</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44856</th>\n",
       "      <td>space jam</td>\n",
       "      <td>[2021-07-03, 2021-07-04, 2021-07-05, 2021-07-06, 2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12, 2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18, 2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-07, 2021-08-08]</td>\n",
       "      <td>we went to see space jam.... lebron james is not the greatest actor in the world but it was still a cute movie</td>\n",
       "      <td>2021-08-07 00:29:43+00:00</td>\n",
       "      <td>1423803505463156738</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44857</th>\n",
       "      <td>space jam</td>\n",
       "      <td>[2021-07-03, 2021-07-04, 2021-07-05, 2021-07-06, 2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12, 2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18, 2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-07, 2021-08-08]</td>\n",
       "      <td>My favorite Space Jam 2 critique wasn't even a direct critique of the movie; it was just some guy complaining about Rick and Morty not swearing in the movie for children.</td>\n",
       "      <td>2021-08-07 00:20:27+00:00</td>\n",
       "      <td>1423801175372087298</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44858</th>\n",
       "      <td>space jam</td>\n",
       "      <td>[2021-07-03, 2021-07-04, 2021-07-05, 2021-07-06, 2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12, 2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18, 2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-07, 2021-08-08]</td>\n",
       "      <td>@James_LeFraud23 He can't be called LeMickey if he's been in a WARNER BROTHERS MOVIE. so thus he's either LeBugs or LeBoyWonder as in Space Jam: A New Legacy, he was dressed up as the sidekick of the Caped Crusader. \\n\\nAlso, this post has nothing to do with LeBron or basketball.</td>\n",
       "      <td>2021-08-07 00:20:05+00:00</td>\n",
       "      <td>1423801080719319046</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44859</th>\n",
       "      <td>space jam</td>\n",
       "      <td>[2021-07-03, 2021-07-04, 2021-07-05, 2021-07-06, 2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12, 2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18, 2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-07, 2021-08-08]</td>\n",
       "      <td>Suicide Squad Is A Must See 🥳 The Kids Definitely Picked An A1 Movie,They made Up For Space Jam 😂</td>\n",
       "      <td>2021-08-07 00:03:10+00:00</td>\n",
       "      <td>1423796822687031298</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44860</th>\n",
       "      <td>space jam</td>\n",
       "      <td>[2021-07-03, 2021-07-04, 2021-07-05, 2021-07-06, 2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12, 2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18, 2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-07, 2021-08-08]</td>\n",
       "      <td>@RyanEldredgeTV @KingJames @spacejammovie Was such a great movie, you used a clip from the original (and only true) space jam movie</td>\n",
       "      <td>2021-08-07 00:02:41+00:00</td>\n",
       "      <td>1423796704067915782</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44861 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Film  \\\n",
       "0           dune   \n",
       "1           dune   \n",
       "2           dune   \n",
       "3           dune   \n",
       "4           dune   \n",
       "...          ...   \n",
       "44856  space jam   \n",
       "44857  space jam   \n",
       "44858  space jam   \n",
       "44859  space jam   \n",
       "44860  space jam   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                         Time Range  \\\n",
       "0                                                                                                                                                                                                                                                                              [2020-09-09, 2020-09-10, 2020-09-11, 2020-09-12, 2020-09-13, 2020-09-14, 2020-09-15, 2020-09-16, 2020-09-17, 2020-09-18, 2020-09-19, 2020-09-20, 2020-09-21, 2020-09-22, 2020-09-23]   \n",
       "1                                                                                                                                                                                                                                                                              [2020-09-09, 2020-09-10, 2020-09-11, 2020-09-12, 2020-09-13, 2020-09-14, 2020-09-15, 2020-09-16, 2020-09-17, 2020-09-18, 2020-09-19, 2020-09-20, 2020-09-21, 2020-09-22, 2020-09-23]   \n",
       "2                                                                                                                                                                                                                                                                              [2020-09-09, 2020-09-10, 2020-09-11, 2020-09-12, 2020-09-13, 2020-09-14, 2020-09-15, 2020-09-16, 2020-09-17, 2020-09-18, 2020-09-19, 2020-09-20, 2020-09-21, 2020-09-22, 2020-09-23]   \n",
       "3                                                                                                                                                                                                                                                                              [2020-09-09, 2020-09-10, 2020-09-11, 2020-09-12, 2020-09-13, 2020-09-14, 2020-09-15, 2020-09-16, 2020-09-17, 2020-09-18, 2020-09-19, 2020-09-20, 2020-09-21, 2020-09-22, 2020-09-23]   \n",
       "4                                                                                                                                                                                                                                                                              [2020-09-09, 2020-09-10, 2020-09-11, 2020-09-12, 2020-09-13, 2020-09-14, 2020-09-15, 2020-09-16, 2020-09-17, 2020-09-18, 2020-09-19, 2020-09-20, 2020-09-21, 2020-09-22, 2020-09-23]   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                             ...   \n",
       "44856  [2021-07-03, 2021-07-04, 2021-07-05, 2021-07-06, 2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12, 2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18, 2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-07, 2021-08-08]   \n",
       "44857  [2021-07-03, 2021-07-04, 2021-07-05, 2021-07-06, 2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12, 2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18, 2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-07, 2021-08-08]   \n",
       "44858  [2021-07-03, 2021-07-04, 2021-07-05, 2021-07-06, 2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12, 2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18, 2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-07, 2021-08-08]   \n",
       "44859  [2021-07-03, 2021-07-04, 2021-07-05, 2021-07-06, 2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12, 2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18, 2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-07, 2021-08-08]   \n",
       "44860  [2021-07-03, 2021-07-04, 2021-07-05, 2021-07-06, 2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12, 2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18, 2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-07, 2021-08-08]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                           Text  \\\n",
       "0                                                  I wanna see Dune but the lack of Muslim and MENA actors when the novel supposedly references the middle-east and Islam is.... disappointing. Would have been a real good bit of representation for once, especially in a big Hollywood movie   \n",
       "1                                                                                    I might be the only person that knows fuckall about Dune but damn that trailer looks great and I love Denis Villenueve. That dude hasn't made a bad movie that I've seen and I've seen almost all of them.   \n",
       "2                                                                                                                                          When people try to claim Dune is some higher level, intelligent adult sci-fi and I wonder if I watched the same movie? #Dune https://t.co/rHiAbRLQL0   \n",
       "3                                                                                                                                                                                                       @nosleeptilltaco I never read Dune but Im v glad yall that did are getting a good movie   \n",
       "4                 @dunemovie The mix of styles is really strange , the music is  not matching the clip or the ambiance , almost disgusting me , also the casting isn't that great imo for a movie with such ambition .. I have that feeling telling me it's gonna be an okay movie and not more   \n",
       "...                                                                                                                                                                                                                                                                                         ...   \n",
       "44856                                                                                                                                                                            we went to see space jam.... lebron james is not the greatest actor in the world but it was still a cute movie   \n",
       "44857                                                                                                                My favorite Space Jam 2 critique wasn't even a direct critique of the movie; it was just some guy complaining about Rick and Morty not swearing in the movie for children.   \n",
       "44858  @James_LeFraud23 He can't be called LeMickey if he's been in a WARNER BROTHERS MOVIE. so thus he's either LeBugs or LeBoyWonder as in Space Jam: A New Legacy, he was dressed up as the sidekick of the Caped Crusader. \\n\\nAlso, this post has nothing to do with LeBron or basketball.   \n",
       "44859                                                                                                                                                                                         Suicide Squad Is A Must See 🥳 The Kids Definitely Picked An A1 Movie,They made Up For Space Jam 😂   \n",
       "44860                                                                                                                                                       @RyanEldredgeTV @KingJames @spacejammovie Was such a great movie, you used a clip from the original (and only true) space jam movie   \n",
       "\n",
       "                       Datetime             Tweet Id Language  \n",
       "0     2020-09-09 23:59:53+00:00  1303845614120304641       en  \n",
       "1     2020-09-09 23:59:48+00:00  1303845592548941824       en  \n",
       "2     2020-09-09 23:59:43+00:00  1303845572814790656       en  \n",
       "3     2020-09-09 23:59:40+00:00  1303845558373756929       en  \n",
       "4     2020-09-09 23:59:39+00:00  1303845555169374214       en  \n",
       "...                         ...                  ...      ...  \n",
       "44856 2021-08-07 00:29:43+00:00  1423803505463156738       en  \n",
       "44857 2021-08-07 00:20:27+00:00  1423801175372087298       en  \n",
       "44858 2021-08-07 00:20:05+00:00  1423801080719319046       en  \n",
       "44859 2021-08-07 00:03:10+00:00  1423796822687031298       en  \n",
       "44860 2021-08-07 00:02:41+00:00  1423796704067915782       en  \n",
       "\n",
       "[44861 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o8HacsFx0L4"
   },
   "source": [
    "## 2.3 - Enregistrement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ak_7ZmVmvtzb"
   },
   "outputs": [],
   "source": [
    "file = os.path.join(\"data\", \"web\", \"web.csv\")\n",
    "if not os.path.exists(os.path.dirname(file)):\n",
    "    os.makedirs(os.path.dirname(file))\n",
    "tweets_df.to_csv(file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Scraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
