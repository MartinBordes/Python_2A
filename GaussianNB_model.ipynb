{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "t = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Création des fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment140(stop_words, lemmatization, negation, nb_data):\n",
    "    file = \"train\"\n",
    "    if stop_words:\n",
    "        file += \"_stop\"\n",
    "    if lemmatization:\n",
    "        file += \"_lemm\"\n",
    "    if negation:\n",
    "        file += \"_neg\"\n",
    "    return pd.read_pickle(os.path.join(\"data\", \"Sentiment140\", file + \".bz2\")).sample(nb_data, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140(False, False, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelisation():\n",
    "    def __init__(self, X, y, vectorizer, model, scaling=True):\n",
    "        X = vectorizer.fit_transform(X)\n",
    "\n",
    "        if scaling:\n",
    "            scaler = StandardScaler(with_mean=False)\n",
    "            X = scaler.fit_transform(X)\n",
    "        \n",
    "        if isinstance(model, GaussianNB):\n",
    "            X = X.toarray()\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=1234)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        sc_accuracy = metrics.accuracy_score(y_pred, y_test)    \n",
    "        sc_balanced_accuracy = metrics.balanced_accuracy_score(y_pred, y_test)\n",
    "        sc_roc_auc = metrics.roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "        probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        metrics_score = {'accuracy': sc_accuracy, 'balanced_accuracy': sc_balanced_accuracy, 'roc_auc': sc_roc_auc}\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = X_train, X_test, y_train, y_test\n",
    "        self.model = model\n",
    "        self.probs = probs\n",
    "        self.metrics_score = metrics_score\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "    \n",
    "    def show_conf_matrix(self):\n",
    "        metrics.plot_confusion_matrix(self.model, self.X_test, self.y_test, cmap='Blues')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Accuracy : {self.metrics_score['accuracy']:.4f}\")\n",
    "        print(f\"Balanced accuracy : {self.metrics_score['balanced_accuracy']:.4f}\")\n",
    "        print(f\"ROC AUC : {self.metrics_score['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparaison(X, y, N, model, scaling=True, show_graph=True):\n",
    "    \"\"\"\n",
    "    Compraison des modèles par rapport à la courbe ROC\n",
    "    N correspond au nombre de features\n",
    "    \"\"\"\n",
    "    table = []\n",
    "    \n",
    "    m = Modelisation(X, y, CountVectorizer(max_features = N), model)\n",
    "    fpr, tpr, _ = metrics.roc_curve(m.y_test, m.probs, pos_label=\"Positive\")\n",
    "    if show_graph: plt.plot(fpr, tpr, label=f\"CountVectorizer {N}\")\n",
    "    table.append(m.metrics_score)\n",
    "\n",
    "    m = Modelisation(X, y, TfidfVectorizer(max_features = N), model)\n",
    "    fpr, tpr, _ = metrics.roc_curve(m.y_test, m.probs, pos_label=\"Positive\")\n",
    "    if show_graph: plt.plot(fpr, tpr, label=f\"TfidfVectorizer {N}\")\n",
    "    table.append(m.metrics_score)\n",
    "\n",
    "    m = Modelisation(X, y, CountVectorizer(max_features = N, ngram_range=(1,2)), model)\n",
    "    fpr, tpr, _ = metrics.roc_curve(m.y_test, m.probs, pos_label=\"Positive\")\n",
    "    if show_graph: plt.plot(fpr, tpr, label=f\"CountVectorizer {N} ngram 2\")\n",
    "    table.append(m.metrics_score)\n",
    "\n",
    "    m = Modelisation(X, y, TfidfVectorizer(max_features = N, ngram_range=(1,2)), model)\n",
    "    fpr, tpr, _ = metrics.roc_curve(m.y_test, m.probs, pos_label=\"Positive\")\n",
    "    if show_graph: plt.plot(fpr, tpr, label=f\"TfidfVectorizer {N} ngram 2\")\n",
    "    table.append(m.metrics_score)\n",
    "    \n",
    "    if show_graph:\n",
    "        plt.plot([0, 1], [0, 1], \"r-\")\n",
    "        plt.plot([0, 0, 1], [0, 1, 1], 'b-')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    df = pd.DataFrame(table, index=[f\"CountVectorizer {N}\", f\"TfidfVectorizer {N}\", f\"CountVectorizer {N} ngram 2\", f\"TfidfVectorizer {N} ngram 2\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def influence_preprocessing(N, model, show_graph=False, show_table=False):\n",
    "    tables = []\n",
    "    for stop_words in [False, True]:\n",
    "        for lemmatization in [False, True]:\n",
    "            for negation in [False, True]:\n",
    "                print(f\"stop_words: {stop_words}, lemmatization: {lemmatization}, negation: {negation}\")\n",
    "                df_temp = sentiment140(stop_words, lemmatization, negation, NB_DATA)\n",
    "                X = df_temp.text.to_list()\n",
    "                y = df_temp.sentiment.to_list()            \n",
    "                df_metrics = comparaison(X, y, N, model, show_graph=show_graph)\n",
    "                if show_table:\n",
    "                    display(df_metrics)\n",
    "                df_metrics['stop_words'] = stop_words\n",
    "                df_metrics['lemmatization'] = lemmatization\n",
    "                df_metrics['negation'] = negation\n",
    "                tables.append(df_metrics)\n",
    "    return pd.concat(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def influence_features(X, y, max_features_list, model, scaling=True):\n",
    "    table = []\n",
    "    for max_features in max_features_list:\n",
    "        m = Modelisation(X, y, CountVectorizer(max_features=max_features), model)\n",
    "        metrics_score = m.metrics_score\n",
    "        metrics_score['features'] = max_features\n",
    "        metrics_score['method'] = \"CountVectorizer\"\n",
    "        table.append(metrics_score)\n",
    "\n",
    "        m = Modelisation(X, y, TfidfVectorizer(max_features=max_features), model)\n",
    "        metrics_score = m.metrics_score\n",
    "        metrics_score['features'] = max_features\n",
    "        metrics_score['method'] = \"TfidfVectorizer\"\n",
    "        table.append(metrics_score)\n",
    "\n",
    "        m = Modelisation(X, y, CountVectorizer(max_features=max_features, ngram_range=(1, 2)), model)\n",
    "        metrics_score = m.metrics_score\n",
    "        metrics_score['features'] = max_features\n",
    "        metrics_score['method'] = \"CountVectorizer ngram 2\"\n",
    "        table.append(metrics_score)\n",
    "\n",
    "        m = Modelisation(X, y, TfidfVectorizer(max_features=max_features, ngram_range=(1, 2)), model)\n",
    "        metrics_score = m.metrics_score\n",
    "        metrics_score['features'] = max_features\n",
    "        metrics_score['method'] = \"TfidfVectorizer ngram 2\"\n",
    "        table.append(metrics_score)\n",
    "\n",
    "    df = pd.DataFrame(table)\n",
    "\n",
    "    methods = list(set(df.method))\n",
    "    scores = list(df.columns[:-2])\n",
    "    fig, axes = plt.subplots(1, len(scores), figsize=(20, 6))\n",
    "    for i_score in range(len(scores)):\n",
    "        for j_method in range(len(methods)):\n",
    "            df[df.method == methods[j_method]].plot(x='features', y=scores[i_score], label=methods[j_method], ax=axes[i_score])\n",
    "        axes[i_score].set(xlabel='Nombre de features')\n",
    "        axes[i_score].legend()\n",
    "        axes[i_score].set_title(scores[i_score])\n",
    "    plt.show()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Sélection des meilleures paramètres du modèle Gaussian naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par choisir le nombre de lignes que l'on extrait de la base Sentiment140 sur laquelle on entraîne et évalue notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_DATA = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Comparaison des méthodes de preprocessing et de feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words: False, lemmatization: False, negation: False\n"
     ]
    }
   ],
   "source": [
    "df_metrics = influence_preprocessing(500, GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.sort_values(by=\"roc_auc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.sort_values(by=\"balanced_accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Comparaison par rapport au nombre de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_list = np.logspace(2.3, 3.7, 6, endpoint=True, dtype=int)\n",
    "influence_features(X, y, max_features_list, GaussianNB())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
