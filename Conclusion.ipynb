{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Préparation du traitement des données"
      ],
      "metadata": {
        "id": "TwJnJMVGzwQC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5tK70FzzYRO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import make_pipeline \n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ro6jkYQzYRS"
      },
      "outputs": [],
      "source": [
        "NB_DATA_140 = 100000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a enregistré plusieurs versions des deux jeux de données préprocessées de manières différentes avec des combinaisons différentes des arguments *stop_words*, *lemmatization* et *negation*. On va créer une fonction *data* qui, en fonction des paramètres de préprocessing choisis par l'utilisateur, chargera les deux dataframes (sentiment140 et tweets scrapés) correspondants."
      ],
      "metadata": {
        "id": "6vjaogXF1gV2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q--nOp-ezYRT"
      },
      "outputs": [],
      "source": [
        "def data(stop_words, lemmatization, negation):\n",
        "    file = \"train\"\n",
        "    if stop_words:\n",
        "        file += \"_stop\"\n",
        "    if lemmatization:\n",
        "        file += \"_lemm\"\n",
        "    if negation:\n",
        "        file += \"_neg\"\n",
        "    df_140 = pd.read_pickle(os.path.join(\"data\", \"sentiment140\", file + \".bz2\")).sample(NB_DATA_140, random_state=1234).reset_index(drop=True)\n",
        "\n",
        "    file = \"web\"\n",
        "    if stop_words:\n",
        "        file += \"_stop\"\n",
        "    if lemmatization:\n",
        "        file += \"_lemm\"\n",
        "    if negation:\n",
        "        file += \"_neg\"\n",
        "    df_web = pd.read_pickle(os.path.join(\"data\", \"web\", file + \".bz2\"))\n",
        "\n",
        "    X_140 = df_140.text.to_list()\n",
        "    y_140 = df_140.sentiment.to_list()\n",
        "\n",
        "    X_web = df_web.Text.to_list()\n",
        "    \n",
        "    return X_140, y_140, X_web, df_web"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va sélectionner le modèle pertinent ainsi que la méthode de feature extraction."
      ],
      "metadata": {
        "id": "6ynSmwbE2HO1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0j6ZiL4zYRU"
      },
      "outputs": [],
      "source": [
        "model = make_pipeline(TfidfVectorizer(max_features=6000, ngram_range=(1,2)), LogisticRegression(max_iter=500))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Application du modèle"
      ],
      "metadata": {
        "id": "9xmD7e1J29VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On choisit nos arguments de préprocessing et on appelle la base pertinente à l’aide de la fonction data"
      ],
      "metadata": {
        "id": "0RYP8AWD3RBy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z-DUTYlzYRU"
      },
      "outputs": [],
      "source": [
        "STOP_WORDS = False\n",
        "LEMMATIZATION = True\n",
        "NEGATION = False\n",
        "X_140, y_140, X_web, df_web = data(STOP_WORDS, LEMMATIZATION, NEGATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application du modèle pour labelliser les tweets scrapés"
      ],
      "metadata": {
        "id": "Q-KbxwGU3Vbh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p06ryRehzYRV"
      },
      "outputs": [],
      "source": [
        "y_web = model.predict(X_web)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ajout de la prédiction au dataframe des tweets"
      ],
      "metadata": {
        "id": "Wsrk8S9Y3wUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZarsmNr0zYRV"
      },
      "outputs": [],
      "source": [
        "df_web['sentiment'] = resul"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Visualisation"
      ],
      "metadata": {
        "id": "bSJS1Ed930UI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nyrnnsNzYRW"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x='sentiment', data=df_web)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj0AO6vdzYRW"
      },
      "outputs": [],
      "source": [
        "sns.histplot(x='Film', hue='sentiment', data=df_web)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JOHDqjhzYRW"
      },
      "outputs": [],
      "source": [
        "df_web['Time Range'] = df_web['Time Range'].replace([\"[2021-10-08, 2021-10-09, 2021-10-10, 2021-10-11, 2021-10-12, 2021-10-13, 2021-10-14, 2021-10-15, 2021-10-16, 2021-10-17, 2021-10-18, 2021-10-19, 2021-10-20, 2021-10-21, 2021-10-22, 2021-10-23, 2021-10-24, 2021-10-25, 2021-10-26, 2021-10-27, 2021-10-28, 2021-10-29, 2021-10-30, 2021-10-31, 2021-11-01, 2021-11-02, 2021-11-03, 2021-11-04, 2021-11-05, 2021-11-06, 2021-11-07, 2021-11-08, 2021-11-09, 2021-11-10, 2021-11-11, 2021-11-12, 2021-11-13]\",\n",
        "                                                     \"[2021-07-03, 2021-07-04, 2021-07-05, 2021-07-06, 2021-07-07, 2021-07-08, 2021-07-09, 2021-07-10, 2021-07-11, 2021-07-12, 2021-07-13, 2021-07-14, 2021-07-15, 2021-07-16, 2021-07-17, 2021-07-18, 2021-07-19, 2021-07-20, 2021-07-21, 2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05, 2021-08-06, 2021-08-07, 2021-08-08]\",\n",
        "                                                     \"[2020-09-09, 2020-09-10, 2020-09-11, 2020-09-12, 2020-09-13, 2020-09-14, 2020-09-15, 2020-09-16, 2020-09-17, 2020-09-18, 2020-09-19, 2020-09-20, 2020-09-21, 2020-09-22, 2020-09-23]\",\n",
        "                                                     \"[2021-07-22, 2021-07-23, 2021-07-24, 2021-07-25, 2021-07-26, 2021-07-27, 2021-07-28, 2021-07-29, 2021-07-30, 2021-07-31, 2021-08-01, 2021-08-02, 2021-08-03, 2021-08-04, 2021-08-05]\",\n",
        "                                                    \"[2021-04-03, 2021-04-04, 2021-04-05, 2021-04-06, 2021-04-07, 2021-04-08, 2021-04-09, 2021-04-10, 2021-04-11, 2021-04-12, 2021-04-13, 2021-04-14, 2021-04-15, 2021-04-16, 2021-04-17]\",\n",
        "                                                    \"[2021-06-09, 2021-06-10, 2021-06-11, 2021-06-12, 2021-06-13, 2021-06-14, 2021-06-15, 2021-06-16, 2021-06-17, 2021-06-18, 2021-06-19, 2021-06-20, 2021-06-21, 2021-06-22, 2021-06-23]\"],\n",
        "                                                    [\"Sortie Dune\", \"Sortie Space Jam\", \"Premier trailer Dune\", \"Second trailer Dune\",\"Premier trailer Space Jam\", \"Second trailer Space Jam\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Kxeq6QMzYRX"
      },
      "outputs": [],
      "source": [
        "sns.histplot(x='Time Range', hue='sentiment', data=df_web.loc[df_web['Film'] == 'dune'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnpvf5vuzYRX"
      },
      "outputs": [],
      "source": [
        "sns.histplot(x='Time Range', hue='sentiment', data=df_web.loc[df_web['Film'] == 'space jam'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7rc94qQzYRY"
      },
      "outputs": [],
      "source": [
        "df_web['day'] = [time.day for time in df_web['Datetime']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N8pyzyvzYRZ"
      },
      "outputs": [],
      "source": [
        "sns.histplot(x='day', hue='sentiment', data=df_web.loc[df_web['Film'] == 'dune'].loc[df_web['Time Range'] == \"Sortie Dune\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIXSIk6BzYRZ"
      },
      "outputs": [],
      "source": [
        "sns.histplot(x='day', hue='sentiment', data=df_web.loc[df_web['Film'] == 'space jam'].loc[df_web['Time Range'] == \"Sortie Space Jam\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Conclusion.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}